{
  "heading": "Writing & Notes",
  "intro": "Technical notes and essays on systems, architecture, and engineering decisions that age well.",
  "items": [
    {
      "id": "ai-systems-production",
      "title": "Designing AI Systems That Don't Rot in Production",
      "premise": "Notes on trade-offs, evaluation drift, and long-term maintainability.",
      "tag": "AI",
      "date": "December 2025",
      "content": "Most AI systems fail not at deployment, but 6 months later when the team has moved on and the model's assumptions have drifted.\n\nThe problem isn't technical sophistication—it's that we treat AI components like deterministic code when they're fundamentally different. A traditional API either works or throws an error. An AI model silently degrades.\n\nHere's what I've learned building production AI systems:\n\n1. Evaluation drift is inevitable\nYour test set represents reality at deployment time. Reality changes. Your model doesn't. The gap grows every day.\n\nSolution: Continuous evaluation on fresh data. Not monthly reports—automated checks that alert when performance drops below thresholds.\n\n2. Prompt engineering creates technical debt\nEvery clever prompt is an undocumented dependency on model behavior. When you upgrade the model, those prompts break in subtle ways.\n\nSolution: Version your prompts. Test them like code. Document the reasoning behind each trick.\n\n3. Fallback strategies matter more than primary strategies\nYour AI will fail. The question is: does it fail gracefully or catastrophically?\n\nSolution: Design fallback paths from day one. Human-in-the-loop for edge cases. Clear confidence thresholds for automatic vs. manual routing.\n\n4. Observability is different for AI\nYou can't just log inputs and outputs. You need to track: confidence scores, latency distributions, cost per request, fallback rates, human override frequency.\n\nSolution: Build AI-specific observability from the start. Make it easy to answer: \"Why did this prediction happen?\" and \"Is performance degrading?\"\n\nThe teams that succeed with production AI treat it like a living system that needs continuous care, not a feature you ship and forget.",
      "url": null
    },
    {
      "id": "mvp-architecture-scale",
      "title": "Why Most MVP Architectures Fail at Team Scale",
      "premise": "A practical look at early decisions that age badly.",
      "tag": "Architecture",
      "date": "November 2025",
      "content": "The MVP worked. You got users. Now you have 5 engineers instead of 2, and everything is slower.\n\nThis isn't a story about technical debt—every fast-moving team has that. This is about architectural decisions that made perfect sense for 2 people but become bottlenecks at 5+.\n\nHere are the patterns I see repeatedly:\n\n1. The Monolithic Database\nEarly on, one database is simple. Everyone knows the schema. Migrations are easy.\n\nAt 5+ engineers: Every migration requires coordination. Schema changes block multiple features. You can't parallelize work because everyone touches the same tables.\n\nThe fix isn't microservices—it's bounded contexts. Separate databases for separate domains. Even in a monolith, you can enforce: \"Auth service only touches auth tables.\"\n\n2. Shared Mutable State\nGlobal state, singleton services, shared caches—these make early development fast. You don't have to think about dependencies.\n\nAt 5+ engineers: Race conditions appear. Bug fixes in one feature break another. Testing becomes a nightmare because everything depends on everything.\n\nThe fix: Immutability by default. Explicit dependency injection. Make side effects visible.\n\n3. \"We'll Add Tests Later\"\nYou shipped without tests because you were moving fast. The code worked, users were happy.\n\nAt 5+ engineers: Nobody knows what will break when they change something. Velocity drops because everyone is scared to touch core systems.\n\nThe fix isn't 100% coverage—it's covering the critical paths first. Can you deploy with confidence? Can new engineers make changes without breaking prod?\n\n4. No Clear Module Boundaries\nIn a 2-person team, everyone knows the whole codebase. File structure doesn't matter much.\n\nAt 5+ engineers: People step on each other's toes. PRs conflict constantly. You can't work in parallel because there's no clear ownership.\n\nThe fix: Organize by domain, not by layer. Create clear interfaces between modules. Make it obvious where code belongs.\n\nThe hard truth: These issues are invisible until they're not. You won't feel the pain at 3 engineers. At 5, it's annoying. At 10, it's a crisis.\n\nThe teams that scale well make these architectural investments early—not for tomorrow's scale, but for next month's team size.",
      "url": null
    },
    {
      "id": "correctness-speed-dichotomy",
      "title": "Correctness vs Speed Is a False Dichotomy",
      "premise": "Applying theoretical thinking to real engineering teams.",
      "tag": "Systems",
      "date": "October 2025",
      "content": "Engineers love to debate: should we move fast or build it right?\n\nThe premise is wrong. Speed and correctness aren't opposites—they're complementary when you build the right abstractions.\n\nHere's what I mean:\n\n1. Type systems make you faster\nYes, writing types takes time upfront. But refactoring with types is 10x faster than refactoring without them.\n\nThe compiler catches your mistakes instantly. You don't waste hours debugging runtime errors that TypeScript would have caught in seconds.\n\n2. Integration tests enable speed\nYes, writing tests takes time. But deploying without tests means:\n- Manual QA on every change\n- Fear of touching core systems\n- Bugs discovered by users, not you\n\nGood tests are an accelerant, not a brake.\n\n3. Clear interfaces enable parallel work\nYes, designing interfaces takes time. But without them:\n- Everyone blocks everyone else\n- You can't split work across the team\n- Integration is a nightmare\n\nClear contracts between components mean 5 engineers can work in parallel instead of serially.\n\n4. Monitoring enables fast iteration\nYes, setting up observability takes time. But without it:\n- You don't know if changes improved things\n- You discover bugs days later\n- You can't make data-driven decisions\n\nGood monitoring turns deployment from \"hope and pray\" to \"measure and iterate.\"\n\nThe pattern: correctness tooling that feels slow upfront pays off exponentially as the team and codebase grow.\n\nThe teams that move fastest long-term are the ones who invested in correctness mechanisms early. They're not moving fast despite correctness—they're moving fast because of it.\n\nSpeed without correctness is just running in circles.",
      "url": null
    },
    {
      "id": "platform-architecture-notes",
      "title": "Architecture Notes from Rebuilding an Internal Platform",
      "premise": "Lessons from scaling under delivery pressure with real constraints.",
      "tag": "Engineering",
      "date": "September 2025",
      "content": "We rebuilt our internal platform while supporting 10+ active projects. Here's what worked and what didn't.\n\nContext: 15-person engineering org, mix of junior and senior engineers, aggressive deadlines, zero downtime tolerance.\n\nWhat Worked:\n\n1. Incremental Migration, Not Big Bang\nWe ran old and new platforms in parallel for 3 months. Projects could opt in when ready.\n\nWhy it worked: No forced migrations. Teams moved when they had capacity. We caught issues with low stakes.\n\n2. API-First Design\nWe designed the API before writing any implementation. Got feedback from 5+ teams before writing code.\n\nWhy it worked: The API was shaped by real use cases, not our assumptions. When we built the backend, we already knew it would fit.\n\n3. Make the Right Thing Easy\nOur old platform required 10 steps to deploy. New platform: one command. We baked best practices into defaults.\n\nWhy it worked: Teams adopted because it was easier, not because we forced them. Education through tooling.\n\nWhat Didn't Work:\n\n1. Perfect Documentation Plans\nWe planned comprehensive docs before launch. Spent weeks writing. Nobody read them.\n\nWhy it failed: Docs get stale instantly. Teams want examples and quick starts, not reference manuals.\n\nWhat we should have done: Inline examples in the code. Auto-generated API docs. Video walkthroughs for common tasks.\n\n2. Feature Parity with Old Platform\nWe tried to match every feature of the old system before launching. Delayed launch by 6 weeks.\n\nWhy it failed: Half those features were unused. We built things nobody wanted.\n\nWhat we should have done: Launch with core features. Add others based on actual demand, not assumptions.\n\n3. Assuming Teams Would Ask Questions\nWe set up office hours. Few people came. Teams struggled silently instead.\n\nWhy it failed: Engineers don't like admitting they're stuck. They'll bang their head for hours rather than ask.\n\nWhat we should have done: Proactive check-ins. Monitor for common error patterns. Reach out when we see teams struggling.\n\nBiggest Lesson:\n\nYou're not building a platform. You're building a migration path.\n\nThe tech is easy. The hard part is getting people to change their workflows while they're under delivery pressure.\n\nSuccess metrics aren't \"features shipped\"—they're \"teams migrated without incident.\"",
      "url": null
    }
  ]
}
